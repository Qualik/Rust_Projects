fn main() {
    // let mut x = 5;
    // println!("The value of x is: {}", x);
    // x = 6;
    // println!("The value of x is: {}", x);

    // let  x = 5;
    // 'mut' not used here, using the same variable name but not the 'mut' keyword

    // let x = x + 1;

    // println!("The value of x is: {}", x);

    // let spaces = "   ";
    // let spaces = spaces.len();

    // let mut spaces = "   ";
    // spaces = spaces.len();

    // let x: i32 = -5;
    // // let x = x + 1;
    // // let x = x * 2;
    // println!("The value of x is: {}", x);

    // let x = 2.0;
    // If I add 'f64' type inference will take precedence over the default-type
    // Type inference assumes it can infer that 'x' is a type 'f32' therefore 'z' will be a type 'f32'
    // If 'f64' is set explicitly then you will see an error in the code
    // as type inference will think its 'f64' and error
    // if 'f64' is removed the error is cleared as 'f32' is then inferred.
    // let y: f32 = 3.0;
    // Type inference would indicate 'z' is an 'f32'
    // let z: f32 = x + y;
    // println!("The value of z is: {}", z);

    // Note: signed = i32 = -128 to 127
    // Note: unsigned = u32 - 0 to 255


}
